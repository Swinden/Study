# 支持向量机

[支持向量机 (tutorialspoint.com)](https://www.tutorialspoint.com/machine_learning_with_python/classification_algorithms_support_vector_machine.htm)

## SVM 简介

支持向量机 （SVM） 是功能强大且灵活的监督式机器学习算法，用于分类和回归。但通常，它们用于分类问题。在20世纪60年代，SVM首次推出，但后来在1990年进行了改进。与其他机器学习算法相比，SVM 具有其独特的实现方式。最近，它们非常受欢迎，因为它们能够处理多个连续和分类变量。

## SVM 的工作原理

SVM模型基本上是多维空间中超平面中不同类的表示。超平面将由 SVM 以迭代方式生成，以便将错误降至最低。SVM 的目标是将数据集划分为类，以找到最大边际超平面 （MMH）。

![边缘](https://www.tutorialspoint.com/machine_learning_with_python/images/margin.jpg)

以下是 SVM 中的重要概念 −

- **支持向量** − 最接近超平面的数据点称为支持向量。分离线将在这些数据点的帮助下定义。
- **超平面** − 正如我们在上图中看到的，它是一个决策平面或空间，它被划分在具有不同类的一组对象之间。
- **Margin** − 它可以被定义为不同类的壁橱数据点上两条线之间的间隙。它可以计算为从线到支持向量的垂直距离。大保证金被认为是好的保证金，小保证金被认为是糟糕的保证金。

SVM的主要目标是将数据集划分为类以找到最大边际超平面（MMH），并且可以在以下两个步骤中完成 -

- 首先，SVM 将以迭代方式生成超平面，以最佳方式隔离类。
- 然后，它将选择正确分隔类的超平面。

## 在 Python 中实现 SVM

为了在Python中实现SVM，我们将从标准库导入开始，如下所示 -

```
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
import seaborn as sns; sns.set()
```

接下来，我们将创建一个示例数据集，其中包含来自sklearn.dataset.sample_generator的线性可分离数据，以便使用 SVM 进行分类 -

```
from sklearn.datasets import make_blobs
X, y = make_blobs(n_samples=100, centers=2, random_state=0, cluster_std=0.50)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer');
```

以下是生成具有 100 个样本和 2 个聚类的示例数据集后的输出 −

![Map](https://www.tutorialspoint.com/machine_learning_with_python/images/map.jpg)

我们知道 SVM 支持判别分类。它通过简单地在二维的情况下找到一条线来划分类，或者在多维的情况下找到流形。它在上述数据集上实现如下 -

```
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')
plt.plot([0.6], [2.1], 'x', color='black', markeredgewidth=4, markersize=12)
for m, b in [(1, 0.65), (0.5, 1.6), (-0.2, 2.9)]:
   plt.plot(xfit, m * xfit + b, '-k')
plt.xlim(-1, 3.5);   
```

输出如下 −

![Cross](https://www.tutorialspoint.com/machine_learning_with_python/images/cross.jpg)

从上面的输出中我们可以看出，有三种不同的分隔符可以完美地区分上述样本。

如前所述，SVM的主要目标是将数据集划分为类以找到最大边际超平面（MMH），因此，我们可以在每条线周围绘制一条宽度的边距，直到最近的点，而不是在类之间绘制一条零线。可以按如下方式完成 -

```
xfit = np.linspace(-1, 3.5)
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')
for m, b, d in [(1, 0.65, 0.33), (0.5, 1.6, 0.55), (-0.2, 2.9, 0.2)]:
    yfit = m * xfit + b
    plt.plot(xfit, yfit, '-k')
    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\
        color='#AAAAAA', alpha=0.4)
plt.xlim(-1, 3.5);
```

![Green](https://www.tutorialspoint.com/machine_learning_with_python/images/green.jpg)

从输出的上图中，我们可以很容易地观察到判别分类器内的“边距”。SVM 将选择最大化边距的线。

接下来，我们将使用Scikit-Learn的支持向量分类器在此数据上训练SVM模型。在这里，我们使用线性核来拟合SVM，如下所示 -

```
from sklearn.svm import SVC # "Support vector classifier"
model = SVC(kernel='linear', C=1E10)
model.fit(X, y)
```

输出如下 −

```
SVC(C=10000000000.0, cache_size=200, class_weight=None, coef0=0.0,
decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
kernel='linear', max_iter=-1, probability=False, random_state=None,
shrinking=True, tol=0.001, verbose=False)
```

现在，为了更好地理解，下面将绘制2D SVC的决策函数 -

```
def decision_function(model, ax=None, plot_support=True):
   if ax is None:
      ax = plt.gca()
   xlim = ax.get_xlim()
   ylim = ax.get_ylim()
```

为了评估模型，我们需要创建网格，如下所示 -

```
x = np.linspace(xlim[0], xlim[1], 30)
y = np.linspace(ylim[0], ylim[1], 30)
Y, X = np.meshgrid(y, x)
xy = np.vstack([X.ravel(), Y.ravel()]).T
P = model.decision_function(xy).reshape(X.shape)
```

接下来，我们需要绘制决策边界和边际，如下所示 -

```
ax.contour(X, Y, P, colors='k',
levels=[-1, 0, 1], alpha=0.5,
linestyles=['--', '-', '--'])
```

现在，类似地绘制支持向量，如下所示 -

```
if plot_support:
   ax.scatter(model.support_vectors_[:, 0],
      model.support_vectors_[:, 1],
      s=300, linewidth=1, facecolors='none');
ax.set_xlim(xlim)
ax.set_ylim(ylim)
```

现在，使用此函数来拟合我们的模型，如下所示 -

```
plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='summer')
decision_function(model);
```

![Yellow](https://www.tutorialspoint.com/machine_learning_with_python/images/yellow.jpg)

从上面的输出中我们可以观察到，SVM分类器拟合到带有边距的数据，即虚线和支持向量，这种拟合的关键元素，触及虚线。这些支持向量点存储在分类器的 support_vectors_ 属性中，如下所示 ：

```
model.support_vectors_
```

输出如下 −

```
array([[0.5323772 , 3.31338909],
[2.11114739, 3.57660449],
[1.46870582, 1.86947425]])
```

## SVM 内核

在实践中，SVM 算法是使用内核实现的，该内核将输入数据空间转换为所需的形式。SVM 使用一种称为核技巧的技术，其中核采用低维输入空间并将其转换为高维空间。简单来说，内核通过添加更多维度，将不可分离的问题转换为可分离的问题。它使SVM更加强大，灵活和准确。以下是 SVM 使用的一些内核类型 -

### 线性核

它可以用作任意两个观测值之间的点积。线性核的公式如下 -

k（x，x我） = 总和（x*x我)

从上面的公式中，我们可以看到两个向量之间的乘积说x和xi是每对输入值的乘法之和。

### 多项式核

它是线性核的更广义形式，可区分曲线或非线性输入空间。以下是多项式核的公式 −

K（x， xi） = 1 + sum（x * xi）^d

这里的d是多项式的次数，我们需要在学习算法中手动指定。

### 径向基函数 （RBF） 核

RBF 核主要用于 SVM 分类，在不确定维空间中映射输入空间。以下公式在数学上解释它 -

K（x，xi） = exp（-gamma * sum（（x – xi^2））

此处，伽玛范围为 0 到 1。我们需要在学习算法中手动指定它。Gamma 的默认值为 0.1。

当我们为线性可分离数据实现SVM时，我们可以在Python中为不可线性可分离的数据实现它。这可以通过使用内核来完成。

### 例

以下是使用内核创建 SVM 分类器的示例。我们将使用来自scikit-learn的iris数据集 -

我们将从导入以下包开始 -

```
import pandas as pd
import numpy as np
from sklearn import svm, datasets
import matplotlib.pyplot as plt
```

现在，我们需要加载输入数据 −

```
iris = datasets.load_iris()
```

从这个数据集中，我们获取前两个特征，如下所示 -

```
X = iris.data[:, :2]
y = iris.target
```

接下来，我们将用原始数据绘制 SVM 边界，如下所示 -

```
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
h = (x_max / x_min)/100
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
   np.arange(y_min, y_max, h))
X_plot = np.c_[xx.ravel(), yy.ravel()]
```

现在，我们需要提供正则化参数的值，如下所示 -

```
C = 1.0
```

接下来，可以按如下方式创建 SVM 分类器对象 ：

Svc_classifier = svm.SVC（kernel='linear'， C=C）.fit（X， y）

```
Z = svc_classifier.predict(X_plot)
Z = Z.reshape(xx.shape)
plt.figure(figsize=(15, 5))
plt.subplot(121)
plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('Support Vector Classifier with linear kernel')
```

### 输出

```
Text(0.5, 1.0, 'Support Vector Classifier with linear kernel')
```

![Curve](https://www.tutorialspoint.com/machine_learning_with_python/images/curve.jpg)

为了使用**rbf**内核创建SVM分类器，我们可以将内核更改为**rbf，**如下所示-

```
Svc_classifier = svm.SVC(kernel='rbf', gamma =‘auto’,C=C).fit(X, y)
Z = svc_classifier.predict(X_plot)
Z = Z.reshape(xx.shape)
plt.figure(figsize=(15, 5))
plt.subplot(121)
plt.contourf(xx, yy, Z, cmap=plt.cm.tab10, alpha=0.3)
plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1)
plt.xlabel('Sepal length')
plt.ylabel('Sepal width')
plt.xlim(xx.min(), xx.max())
plt.title('Support Vector Classifier with rbf kernel')
```

### 输出

```
Text(0.5, 1.0, 'Support Vector Classifier with rbf kernel')
```

![Classifier](https://www.tutorialspoint.com/machine_learning_with_python/images/classifier.jpg)

​	我们将 gamma 的值设置为“auto”，但您也可以提供介于 0 到 1 之间的值。

## SVM 分类器的优缺点

### SVM 分类器的优点

​	SVM 分类器提供极高的精度，并且可在高维空间下正常工作。SVM分类器基本上使用训练点的子集，因此结果使用非常少的内存。

### SVM 分类器的缺点

​	它们具有很高的训练时间，因此在实践中不适合大型数据集。另一个缺点是 SVM 分类器不能很好地处理重叠类。