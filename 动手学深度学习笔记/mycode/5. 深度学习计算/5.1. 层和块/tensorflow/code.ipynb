{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94983043-eec4-4ef0-acd9-2c12bad270df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../../../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516a014b-d40c-4a71-b80c-ecc8c09bcc0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[ 0.00612219, -0.09744787,  0.1827062 ,  0.03598773, -0.0197178 ,\n",
       "        -0.05299428, -0.02697484,  0.40382344, -0.08403043, -0.23880604],\n",
       "       [ 0.04938347,  0.05792264,  0.03699479,  0.08408086,  0.06871971,\n",
       "         0.11331593, -0.05977355,  0.12896264,  0.19274312, -0.17330498]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "net = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "])\n",
    "\n",
    "X = tf.random.uniform((2, 20))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66961690-747d-41cc-b06e-dcf805198639",
   "metadata": {},
   "source": [
    "1. 将输入数据作为其前向传播函数的参数。\n",
    "\n",
    "2. 通过前向传播函数来生成输出。请注意，输出的形状可能与输入的形状不同。例如，我们上面模型中的第一个全连接的层接收任意维的输入，但是返回一个维度256的输出。\n",
    "\n",
    "3. 计算其输出关于输入的梯度，可通过其反向传播函数进行访问。通常这是自动发生的。\n",
    "\n",
    "4. 存储和访问前向传播计算所需的参数。\n",
    "\n",
    "5. 根据需要初始化模型参数。\n",
    "\n",
    "在下面的代码片段中，我们从零开始编写一个块。 它包含一个多层感知机，其具有256个隐藏单元的隐藏层和一个10维输出层。 注意，下面的MLP类继承了表示块的类。 我们的实现只需要提供我们自己的构造函数（Python中的__init__函数）和前向传播函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "addcd91f-1e37-472c-b765-2ac923ea8fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    # 用模型参数声明层。这里，我们声明两个全连接的层\n",
    "    def __init__(self):\n",
    "        # 调用MLP的父类Model的构造函数来执行必要的初始化。\n",
    "        # 这样，在类实例化时也可以指定其他函数参数，例如模型参数params（稍后将介绍）\n",
    "        super().__init__()\n",
    "        # Hiddenlayer\n",
    "        self.hidden = tf.keras.layers.Dense(units=256, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(units=10)  # Outputlayer\n",
    "\n",
    "    # 定义模型的前向传播，即如何根据输入X返回所需的模型输出\n",
    "    def call(self, X):\n",
    "        return self.out(self.hidden((X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a8f07f6-cb0d-4d3d-be1f-01e72e7a8acc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.11527389,  0.02360106,  0.21813378,  0.06761041, -0.01841843,\n",
       "        -0.07789095,  0.10729354, -0.07375975,  0.29580238, -0.02189789],\n",
       "       [-0.00213279, -0.18418431,  0.14592703, -0.09252997,  0.06848057,\n",
       "         0.00418149,  0.01208355, -0.21147642,  0.21414706,  0.02813512]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0205f7fa-d222-4aa4-97cc-77960f76029f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySequential(tf.keras.Model):\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        self.modules = []\n",
    "        for block in args:\n",
    "            # 这里，block是tf.keras.layers.Layer子类的一个实例\n",
    "            self.modules.append(block)\n",
    "\n",
    "    def call(self, X):\n",
    "        for module in self.modules:\n",
    "            X = module(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9228d115-352d-41b8-a8f5-541450861f69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.06219149,  0.00616149, -0.00230183, -0.27809718,  0.2684607 ,\n",
       "        -0.10135669, -0.22992602,  0.14410971,  0.18993138, -0.0840214 ],\n",
       "       [ 0.06620149, -0.31547397,  0.00817438, -0.16213295,  0.29391223,\n",
       "         0.00760097, -0.11794727, -0.00080982,  0.21800353, -0.04877056]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential(\n",
    "    tf.keras.layers.Dense(units=256, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb154081-b034-4be7-8e3f-5e5de4ebc39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        # 使用tf.constant函数创建的随机权重参数在训练期间不会更新（即为常量参数）\n",
    "        self.rand_weight = tf.constant(tf.random.uniform((20, 20)))\n",
    "        self.dense = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        X = self.flatten(inputs)\n",
    "        # 使用创建的常量参数以及relu和matmul函数\n",
    "        X = tf.nn.relu(tf.matmul(X, self.rand_weight) + 1)\n",
    "        # 复用全连接层。这相当于两个全连接层共享参数。\n",
    "        X = self.dense(X)\n",
    "        # 控制流\n",
    "        while tf.reduce_sum(tf.math.abs(X)) > 1:\n",
    "            X /= 2\n",
    "        return tf.reduce_sum(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe970c6-ade1-4ae8-8a0c-bf4fb4d50622",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.58359927>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55630308-08d2-4292-ab70-fca5720c9a49",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.5557022>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = tf.keras.Sequential()\n",
    "        self.net.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "        self.net.add(tf.keras.layers.Dense(32, activation=tf.nn.relu))\n",
    "        self.dense = tf.keras.layers.Dense(16, activation=tf.nn.relu)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(self.net(inputs))\n",
    "\n",
    "chimera = tf.keras.Sequential()\n",
    "chimera.add(NestMLP())\n",
    "chimera.add(tf.keras.layers.Dense(20))\n",
    "chimera.add(FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266f998-d29f-492d-94e0-bc879179a570",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
