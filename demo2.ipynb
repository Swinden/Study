{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "demo2.ipynb",
      "mount_file_id": "1Gn7GUET8LSxa5NeXbDwiTnoX7i-qpkWs",
      "authorship_tag": "ABX9TyOszDMKVbEz0WBoxMxkt8Rj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Swinden/Study/blob/main/demo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83KnH2Bvly5W",
        "outputId": "36b03778-ef6b-423d-e6e4-d012dd78a177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#导入前置依赖\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# 用于加载bert模型的分词器\n",
        "from transformers import AutoTokenizer\n",
        "# 用于加载bert模型\n",
        "from transformers import BertModel\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "tHZEfI1xh_jM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "# 文本的最大长度\n",
        "text_max_length = 300\n",
        "# 总训练的epochs数，我只是随便定义了个数\n",
        "epochs = 20\n",
        "# 学习率\n",
        "lr = 5e-6\n",
        "# 取多少训练集的数据作为验证集\n",
        "validation_ratio = 0.11\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 每多少步，打印一次loss\n",
        "log_per_step = 50\n",
        "\n",
        "# 数据集所在位置\n",
        "dataset_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/data1\")\n",
        "os.makedirs(dataset_dir) if not os.path.exists(dataset_dir) else ''\n",
        "\n",
        "# 模型存储路径\n",
        "model_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/model/bert_checkpoints\")\n",
        "# 如果模型目录不存在，则创建一个\n",
        "os.makedirs(model_dir) if not os.path.exists(model_dir) else ''\n",
        "\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K_soaDYiaSM",
        "outputId": "daacfc51-f733-40a2-b5ae-c25074af0c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 读取数据集，进行数据处理\n",
        "\n",
        "pd_train_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data1/train.csv')\n",
        "pd_train_data['title'] = pd_train_data['title'].fillna('')\n",
        "pd_train_data['abstract'] = pd_train_data['abstract'].fillna('')\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data2/testB.csv')\n",
        "test_data['title'] = test_data['title'].fillna('')\n",
        "test_data['abstract'] = test_data['abstract'].fillna('')\n",
        "pd_train_data['text'] = pd_train_data['title'].fillna('') + ' ' +  pd_train_data['author'].fillna('') + ' ' + pd_train_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n",
        "test_data['text'] = test_data['title'].fillna('') + ' ' +  test_data['author'].fillna('') + ' ' + test_data['abstract'].fillna('')+ ' ' + pd_train_data['Keywords'].fillna('')\n"
      ],
      "metadata": {
        "id": "fHLFg5GCi4P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 从训练集中随机采样测试集\n",
        "validation_data = pd_train_data.sample(frac=validation_ratio)\n",
        "train_data = pd_train_data[~pd_train_data.index.isin(validation_data.index)]"
      ],
      "metadata": {
        "id": "DAIjQyMXjVwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 构建Dataset\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "    def __init__(self, mode='train'):\n",
        "        super(MyDataset, self).__init__()\n",
        "        self.mode = mode\n",
        "        # 拿到对应的数据\n",
        "        if mode == 'train':\n",
        "            self.dataset = train_data\n",
        "        elif mode == 'validation':\n",
        "            self.dataset = validation_data\n",
        "        elif mode == 'test':\n",
        "            # 如果是测试模式，则返回内容和uuid。拿uuid做target主要是方便后面写入结果。\n",
        "            self.dataset = test_data\n",
        "        else:\n",
        "            raise Exception(\"Unknown mode {}\".format(mode))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # 取第index条\n",
        "        data = self.dataset.iloc[index]\n",
        "        # 取其内容\n",
        "        text = data['text']\n",
        "        # 根据状态返回内容\n",
        "        if self.mode == 'test':\n",
        "            # 如果是test，将uuid做为target\n",
        "            label = data['uuid']\n",
        "        else:\n",
        "            label = data['label']\n",
        "        # 返回内容和label\n",
        "        return text, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n"
      ],
      "metadata": {
        "id": "CxBkA292jXTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MyDataset('train')\n",
        "validation_dataset = MyDataset('validation')"
      ],
      "metadata": {
        "id": "YnjOs-CDjY_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.__getitem__(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jX03F2ijY5y",
        "outputId": "39d2f65d-1e15-4c33-921d-2e84072513b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Accessible Visual Artworks for Blind and Visually Impaired People: Comparing a Multimodal Approach with Tactile Graphics Quero, Luis Cavazos; Bartolome, Jorge Iranzo; Cho, Jundong Despite the use of tactile graphics and audio guides, blind and visually impaired people still face challenges to experience and understand visual artworks independently at art exhibitions. Art museums and other art places are increasingly exploring the use of interactive guides to make their collections more accessible. In this work, we describe our approach to an interactive multimodal guide prototype that uses audio and tactile modalities to improve the autonomous access to information and experience of visual artworks. The prototype is composed of a touch-sensitive 2.5D artwork relief model that can be freely explored by touch. Users can access localized verbal descriptions and audio by performing touch gestures on the surface while listening to themed background music along. We present the design requirements derived from a formative study realized with the help of eight blind and visually impaired participants, art museum and gallery staff, and artists. We extended the formative study by organizing two accessible art exhibitions. There, eighteen participants evaluated and compared multimodal and tactile graphic accessible exhibits. Results from a usability survey indicate that our multimodal approach is simple, easy to use, and improves confidence and independence when exploring visual artworks. accessibility technology; multimodal interaction; auditory interface; touch interface; vision impairment',\n",
              " 0)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#获取Bert预训练模型\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "tX96DcFGjY22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#接着构造我们的Dataloader。\n",
        "#我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    将一个batch的文本句子转成tensor，并组成batch。\n",
        "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
        "    :return: 处理后的结果，例如：\n",
        "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
        "             target：[1, 1, 0, ...]\n",
        "    \"\"\"\n",
        "    text, label = zip(*batch)\n",
        "    text, label = list(text), list(label)\n",
        "\n",
        "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
        "    # padding='max_length' 不够长度的进行填充\n",
        "    # truncation=True 长度过长的进行裁剪\n",
        "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
        "\n",
        "    return src, torch.LongTensor(label)"
      ],
      "metadata": {
        "id": "EN2WivbtjYzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#接着构造我们的Dataloader。\n",
        "#我们需要定义一下collate_fn，在其中完成对句子进行编码、填充、组装batch等动作：\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    将一个batch的文本句子转成tensor，并组成batch。\n",
        "    :param batch: 一个batch的句子，例如: [('推文', target), ('推文', target), ...]\n",
        "    :return: 处理后的结果，例如：\n",
        "             src: {'input_ids': tensor([[ 101, ..., 102, 0, 0, ...], ...]), 'attention_mask': tensor([[1, ..., 1, 0, ...], ...])}\n",
        "             target：[1, 1, 0, ...]\n",
        "    \"\"\"\n",
        "    text, label = zip(*batch)\n",
        "    text, label = list(text), list(label)\n",
        "\n",
        "    # src是要送给bert的，所以不需要特殊处理，直接用tokenizer的结果即可\n",
        "    # padding='max_length' 不够长度的进行填充\n",
        "    # truncation=True 长度过长的进行裁剪\n",
        "    src = tokenizer(text, padding='max_length', max_length=text_max_length, return_tensors='pt', truncation=True)\n",
        "\n",
        "    return src, torch.LongTensor(label)"
      ],
      "metadata": {
        "id": "z31lylyKjYwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "NnFB681bjYqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, targets = next(iter(train_loader))\n",
        "print(\"inputs:\", inputs)\n",
        "print(\"targets:\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czFo5N78jYhY",
        "outputId": "c65ca366-c903-42e5-dc2f-8f7c713ab21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: {'input_ids': tensor([[  101, 26242,  7722,  ...,  1043,  1011,   102],\n",
            "        [  101,  1996,  2373,  ...,     0,     0,     0],\n",
            "        [  101,  2079, 18923,  ...,  1006,  1038,   102],\n",
            "        ...,\n",
            "        [  101,  6490,  3120,  ...,     0,     0,     0],\n",
            "        [  101, 10723, 18440,  ...,  2475,  1998,   102],\n",
            "        [  101,  8382, 11326,  ...,  1016, 12978,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0],\n",
            "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
            "targets: tensor([1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#定义预测模型，该模型由bert模型加上最后的预测层组成\n",
        "class MyModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "        # 加载bert模型\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased', mirror='tuna')\n",
        "\n",
        "        # 最后的预测层\n",
        "        self.predictor = nn.Sequential(\n",
        "            nn.Linear(768, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, src):\n",
        "        \"\"\"\n",
        "        :param src: 分词后的推文数据\n",
        "        \"\"\"\n",
        "\n",
        "        # 将src直接序列解包传入bert，因为bert和tokenizer是一套的，所以可以这么做。\n",
        "        # 得到encoder的输出，用最前面[CLS]的输出作为最终线性层的输入\n",
        "        outputs = self.bert(**src).last_hidden_state[:, 0, :]\n",
        "\n",
        "        # 使用线性层来做最终的预测\n",
        "        return self.predictor(outputs)\n"
      ],
      "metadata": {
        "id": "WVO5LaHWj1XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "2k9TRbxSj1UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义出损失函数和优化器。这里使用Binary Cross Entropy：\n",
        "criteria = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "zl1qlJZDj1RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 由于inputs是字典类型的，定义一个辅助函数帮助to(device)\n",
        "def to_device(dict_tensors):\n",
        "    result_tensors = {}\n",
        "    for key, value in dict_tensors.items():\n",
        "        result_tensors[key] = value.to(device)\n",
        "    return result_tensors"
      ],
      "metadata": {
        "id": "x-MHjj2rj1LA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#定义一个验证方法，获取到验证集的精准率和loss\n",
        "def validate():\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    total_correct = 0\n",
        "    for inputs, targets in validation_loader:\n",
        "        inputs, targets = to_device(inputs), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criteria(outputs.view(-1), targets.float())\n",
        "        total_loss += float(loss)\n",
        "\n",
        "        correct_num = (((outputs >= 0.5).float() * 1).flatten() == targets).sum()\n",
        "        total_correct += correct_num\n",
        "\n",
        "    return total_correct / len(validation_dataset), total_loss / len(validation_dataset)"
      ],
      "metadata": {
        "id": "4jKAfYIlj1H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 首先将模型调成训练模式\n",
        "model.train()\n",
        "\n",
        "# 清空一下cuda缓存\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# 定义几个变量，帮助打印loss\n",
        "total_loss = 0.\n",
        "# 记录步数\n",
        "step = 0\n",
        "\n",
        "# 记录在验证集上最好的准确率\n",
        "best_accuracy = 0\n",
        "\n",
        "# 开始训练\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        # 从batch中拿到训练数据\n",
        "        inputs, targets = to_device(inputs), targets.to(device)\n",
        "        # 传入模型进行前向传递\n",
        "        outputs = model(inputs)\n",
        "        # 计算损失\n",
        "        loss = criteria(outputs.view(-1), targets.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        step += 1\n",
        "\n",
        "        if step % log_per_step == 0:\n",
        "            print(\"Epoch {}/{}, Step: {}/{}, total loss:{:.4f}\".format(epoch+1, epochs, i, len(train_loader), total_loss))\n",
        "            total_loss = 0\n",
        "\n",
        "        del inputs, targets\n",
        "\n",
        "    # 一个epoch后，使用过验证集进行验证\n",
        "    accuracy, validation_loss = validate()\n",
        "    print(\"Epoch {}, accuracy: {:.4f}, validation loss: {:.4f}\".format(epoch+1, accuracy, validation_loss))\n",
        "    # torch.save(model, model_dir / f\"model_{epoch}.pt\")\n",
        "\n",
        "    # 保存最好的模型\n",
        "    if accuracy > best_accuracy:\n",
        "        torch.save(model, model_dir / f\"model_best.pt\")\n",
        "        best_accuracy = accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYRPA_jOj0-t",
        "outputId": "f41af45d-1694-43f2-fbf5-585ab6bc8f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Step: 49/334, total loss:28.7035\n",
            "Epoch 1/20, Step: 99/334, total loss:17.0512\n",
            "Epoch 1/20, Step: 149/334, total loss:9.9987\n",
            "Epoch 1/20, Step: 199/334, total loss:7.4437\n",
            "Epoch 1/20, Step: 249/334, total loss:5.9816\n",
            "Epoch 1/20, Step: 299/334, total loss:4.4325\n",
            "Epoch 1, accuracy: 0.9758, validation loss: 0.0047\n",
            "Epoch 2/20, Step: 15/334, total loss:3.9038\n",
            "Epoch 2/20, Step: 65/334, total loss:2.7194\n",
            "Epoch 2/20, Step: 115/334, total loss:4.1560\n",
            "Epoch 2/20, Step: 165/334, total loss:2.8484\n",
            "Epoch 2/20, Step: 215/334, total loss:2.6513\n",
            "Epoch 2/20, Step: 265/334, total loss:1.5685\n",
            "Epoch 2/20, Step: 315/334, total loss:2.0519\n",
            "Epoch 2, accuracy: 0.9924, validation loss: 0.0021\n",
            "Epoch 3/20, Step: 31/334, total loss:1.6185\n",
            "Epoch 3/20, Step: 81/334, total loss:1.7487\n",
            "Epoch 3/20, Step: 131/334, total loss:1.7981\n",
            "Epoch 3/20, Step: 181/334, total loss:2.0413\n",
            "Epoch 3/20, Step: 231/334, total loss:1.4561\n",
            "Epoch 3/20, Step: 281/334, total loss:1.5774\n",
            "Epoch 3/20, Step: 331/334, total loss:3.1136\n",
            "Epoch 3, accuracy: 0.9894, validation loss: 0.0024\n",
            "Epoch 4/20, Step: 47/334, total loss:0.8745\n",
            "Epoch 4/20, Step: 97/334, total loss:0.9890\n",
            "Epoch 4/20, Step: 147/334, total loss:0.9771\n",
            "Epoch 4/20, Step: 197/334, total loss:1.0884\n",
            "Epoch 4/20, Step: 247/334, total loss:1.1738\n",
            "Epoch 4/20, Step: 297/334, total loss:1.8515\n",
            "Epoch 4, accuracy: 0.9924, validation loss: 0.0020\n",
            "Epoch 5/20, Step: 13/334, total loss:1.2465\n",
            "Epoch 5/20, Step: 63/334, total loss:1.6269\n",
            "Epoch 5/20, Step: 113/334, total loss:0.9272\n",
            "Epoch 5/20, Step: 163/334, total loss:0.6159\n",
            "Epoch 5/20, Step: 213/334, total loss:0.8893\n",
            "Epoch 5/20, Step: 263/334, total loss:0.8607\n",
            "Epoch 5/20, Step: 313/334, total loss:0.4094\n",
            "Epoch 5, accuracy: 0.9924, validation loss: 0.0018\n",
            "Epoch 6/20, Step: 29/334, total loss:0.5649\n",
            "Epoch 6/20, Step: 79/334, total loss:0.4152\n",
            "Epoch 6/20, Step: 129/334, total loss:0.5723\n",
            "Epoch 6/20, Step: 179/334, total loss:1.2867\n",
            "Epoch 6/20, Step: 229/334, total loss:0.7469\n",
            "Epoch 6/20, Step: 279/334, total loss:0.5286\n",
            "Epoch 6/20, Step: 329/334, total loss:1.2329\n",
            "Epoch 6, accuracy: 0.9894, validation loss: 0.0028\n",
            "Epoch 7/20, Step: 45/334, total loss:0.9239\n",
            "Epoch 7/20, Step: 95/334, total loss:0.3741\n",
            "Epoch 7/20, Step: 145/334, total loss:0.2148\n",
            "Epoch 7/20, Step: 195/334, total loss:0.5725\n",
            "Epoch 7/20, Step: 245/334, total loss:0.6707\n",
            "Epoch 7/20, Step: 295/334, total loss:0.2136\n",
            "Epoch 7, accuracy: 0.9848, validation loss: 0.0029\n",
            "Epoch 8/20, Step: 11/334, total loss:0.2001\n",
            "Epoch 8/20, Step: 61/334, total loss:0.5363\n",
            "Epoch 8/20, Step: 111/334, total loss:0.1452\n",
            "Epoch 8/20, Step: 161/334, total loss:0.1248\n",
            "Epoch 8/20, Step: 211/334, total loss:0.1217\n",
            "Epoch 8/20, Step: 261/334, total loss:0.1114\n",
            "Epoch 8/20, Step: 311/334, total loss:0.0997\n",
            "Epoch 8, accuracy: 0.9924, validation loss: 0.0024\n",
            "Epoch 9/20, Step: 27/334, total loss:0.9278\n",
            "Epoch 9/20, Step: 77/334, total loss:0.1233\n",
            "Epoch 9/20, Step: 127/334, total loss:0.0896\n",
            "Epoch 9/20, Step: 177/334, total loss:0.1119\n",
            "Epoch 9/20, Step: 227/334, total loss:0.0890\n",
            "Epoch 9/20, Step: 277/334, total loss:0.0856\n",
            "Epoch 9/20, Step: 327/334, total loss:1.4921\n",
            "Epoch 9, accuracy: 0.9955, validation loss: 0.0012\n",
            "Epoch 10/20, Step: 43/334, total loss:0.4781\n",
            "Epoch 10/20, Step: 93/334, total loss:0.1065\n",
            "Epoch 10/20, Step: 143/334, total loss:0.8007\n",
            "Epoch 10/20, Step: 193/334, total loss:0.1038\n",
            "Epoch 10/20, Step: 243/334, total loss:0.1077\n",
            "Epoch 10/20, Step: 293/334, total loss:0.0845\n",
            "Epoch 10, accuracy: 0.9955, validation loss: 0.0013\n",
            "Epoch 11/20, Step: 9/334, total loss:0.1223\n",
            "Epoch 11/20, Step: 59/334, total loss:0.0765\n",
            "Epoch 11/20, Step: 109/334, total loss:0.4487\n",
            "Epoch 11/20, Step: 159/334, total loss:0.6013\n",
            "Epoch 11/20, Step: 209/334, total loss:0.1485\n",
            "Epoch 11/20, Step: 259/334, total loss:0.1528\n",
            "Epoch 11/20, Step: 309/334, total loss:0.3091\n",
            "Epoch 11, accuracy: 0.9939, validation loss: 0.0021\n",
            "Epoch 12/20, Step: 25/334, total loss:0.0799\n",
            "Epoch 12/20, Step: 75/334, total loss:0.0626\n",
            "Epoch 12/20, Step: 125/334, total loss:0.1733\n",
            "Epoch 12/20, Step: 175/334, total loss:0.1606\n",
            "Epoch 12/20, Step: 225/334, total loss:0.3536\n",
            "Epoch 12/20, Step: 275/334, total loss:1.0122\n",
            "Epoch 12/20, Step: 325/334, total loss:0.6052\n",
            "Epoch 12, accuracy: 0.9894, validation loss: 0.0034\n",
            "Epoch 13/20, Step: 41/334, total loss:0.2023\n",
            "Epoch 13/20, Step: 91/334, total loss:0.0702\n",
            "Epoch 13/20, Step: 141/334, total loss:0.0708\n",
            "Epoch 13/20, Step: 191/334, total loss:0.0658\n",
            "Epoch 13/20, Step: 241/334, total loss:0.0516\n",
            "Epoch 13/20, Step: 291/334, total loss:0.0593\n",
            "Epoch 13, accuracy: 0.9939, validation loss: 0.0021\n",
            "Epoch 14/20, Step: 7/334, total loss:0.3641\n",
            "Epoch 14/20, Step: 57/334, total loss:0.0473\n",
            "Epoch 14/20, Step: 107/334, total loss:0.0472\n",
            "Epoch 14/20, Step: 157/334, total loss:0.6076\n",
            "Epoch 14/20, Step: 207/334, total loss:0.0710\n",
            "Epoch 14/20, Step: 257/334, total loss:0.0602\n",
            "Epoch 14/20, Step: 307/334, total loss:0.0465\n",
            "Epoch 14, accuracy: 0.9924, validation loss: 0.0019\n",
            "Epoch 15/20, Step: 23/334, total loss:0.2737\n",
            "Epoch 15/20, Step: 73/334, total loss:0.0510\n",
            "Epoch 15/20, Step: 123/334, total loss:0.0388\n",
            "Epoch 15/20, Step: 173/334, total loss:0.0356\n",
            "Epoch 15/20, Step: 223/334, total loss:0.1216\n",
            "Epoch 15/20, Step: 273/334, total loss:0.0524\n",
            "Epoch 15/20, Step: 323/334, total loss:0.0385\n",
            "Epoch 15, accuracy: 0.9924, validation loss: 0.0021\n",
            "Epoch 16/20, Step: 39/334, total loss:0.0294\n",
            "Epoch 16/20, Step: 89/334, total loss:0.0300\n",
            "Epoch 16/20, Step: 139/334, total loss:0.0402\n",
            "Epoch 16/20, Step: 189/334, total loss:0.0298\n",
            "Epoch 16/20, Step: 239/334, total loss:0.0265\n",
            "Epoch 16/20, Step: 289/334, total loss:0.0806\n",
            "Epoch 16, accuracy: 0.9939, validation loss: 0.0019\n",
            "Epoch 17/20, Step: 5/334, total loss:0.0271\n",
            "Epoch 17/20, Step: 55/334, total loss:0.0244\n",
            "Epoch 17/20, Step: 105/334, total loss:0.0231\n",
            "Epoch 17/20, Step: 155/334, total loss:0.0218\n",
            "Epoch 17/20, Step: 205/334, total loss:0.0540\n",
            "Epoch 17/20, Step: 255/334, total loss:0.0222\n",
            "Epoch 17/20, Step: 305/334, total loss:0.0212\n",
            "Epoch 17, accuracy: 0.9939, validation loss: 0.0022\n",
            "Epoch 18/20, Step: 21/334, total loss:0.0193\n",
            "Epoch 18/20, Step: 71/334, total loss:0.0190\n",
            "Epoch 18/20, Step: 121/334, total loss:0.0197\n",
            "Epoch 18/20, Step: 171/334, total loss:0.0188\n",
            "Epoch 18/20, Step: 221/334, total loss:0.0169\n",
            "Epoch 18/20, Step: 271/334, total loss:0.0167\n",
            "Epoch 18/20, Step: 321/334, total loss:0.0535\n",
            "Epoch 18, accuracy: 0.9939, validation loss: 0.0022\n",
            "Epoch 19/20, Step: 37/334, total loss:0.0309\n",
            "Epoch 19/20, Step: 87/334, total loss:0.0240\n",
            "Epoch 19/20, Step: 137/334, total loss:0.0152\n",
            "Epoch 19/20, Step: 187/334, total loss:0.0193\n",
            "Epoch 19/20, Step: 237/334, total loss:0.0157\n",
            "Epoch 19/20, Step: 287/334, total loss:0.0132\n",
            "Epoch 19, accuracy: 0.9939, validation loss: 0.0024\n",
            "Epoch 20/20, Step: 3/334, total loss:0.0188\n",
            "Epoch 20/20, Step: 53/334, total loss:0.0125\n",
            "Epoch 20/20, Step: 103/334, total loss:0.0122\n",
            "Epoch 20/20, Step: 153/334, total loss:0.0118\n",
            "Epoch 20/20, Step: 203/334, total loss:0.0116\n",
            "Epoch 20/20, Step: 253/334, total loss:0.0117\n",
            "Epoch 20/20, Step: 303/334, total loss:0.0115\n",
            "Epoch 20, accuracy: 0.9939, validation loss: 0.0026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#加载最好的模型，然后进行测试集的预测\n",
        "model = torch.load(model_dir / f\"model_best.pt\")\n",
        "model = model.eval()"
      ],
      "metadata": {
        "id": "xTDZEqudj07l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = MyDataset('test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "oTEF7Pi9j01M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for inputs, ids in test_loader:\n",
        "    outputs = model(inputs.to(device))\n",
        "    outputs = (outputs >= 0.5).int().flatten().tolist()\n",
        "    ids = ids.tolist()\n",
        "    results = results + [(id, result) for result, id in zip(outputs, ids)]"
      ],
      "metadata": {
        "id": "w9egIE0pj0vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = [pair[1] for pair in results]\n",
        "test_data['label'] = test_label\n",
        "test_data[['uuid', 'label']].to_csv('/content/drive/MyDrive/Colab Notebooks/submit_task1.csv', index=None)"
      ],
      "metadata": {
        "id": "L7jMDBAdj0sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h_--XTUBX6t",
        "outputId": "653b0283-524e-4cc2-928f-7423011e816b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence-transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入pandas用于读取表格数据\n",
        "import pandas as pd\n",
        "\n",
        "# 导入BOW（词袋模型），可以选择将CountVectorizer替换为TfidfVectorizer（TF-IDF（词频-逆文档频率）），注意上下文要同时修改，亲测后者效果更佳\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# 导入Bert模型\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 导入计算相似度前置库，为了计算候选者和文档之间的相似度，我们将使用向量之间的余弦相似度，因为它在高维度下表现得相当好。\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 过滤警告消息\n",
        "from warnings import simplefilter\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n"
      ],
      "metadata": {
        "id": "Gw1qG6V1kLBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 读取数据集\n",
        "# test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data2/testB.csv')\n",
        "\n",
        "test = test_data\n",
        "test['title'] = test['title'].fillna('')\n",
        "test['abstract'] = test['abstract'].fillna('')\n",
        "\n",
        "test['text'] = test['title'].fillna('') + ' ' +test['abstract'].fillna('')"
      ],
      "metadata": {
        "id": "UXx0ppOSj0pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c5IFm7MdEqRk",
        "outputId": "31ad722e-d69e-48d0-80ea-a13147ecdffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      uuid                                              title  \\\n",
              "0        0  Tobacco Consumption and High-Sensitivity Cardi...   \n",
              "1        1  Approaching towards sustainable supply chain u...   \n",
              "2        2  Does globalization matter for ecological footp...   \n",
              "3        3  Myths and Misconceptions About University Stud...   \n",
              "4        4  Antioxidant Status of Rat Liver Mitochondria u...   \n",
              "...    ...                                                ...   \n",
              "1995  1995  The treatment of veterinary antibiotics in swi...   \n",
              "1996  1996  Socio-political efficacy explains increase in ...   \n",
              "1997  1997  Investigation of early puberty prevalence and ...   \n",
              "1998  1998  From 3D printing to 3D bioprinting: the materi...   \n",
              "1999  1999  Effect of Processing on the Structure and Alle...   \n",
              "\n",
              "                                                 author  \\\n",
              "0     Julia Brox Skranes,Magnus Nakrem Lyngbakken,Kr...   \n",
              "1     Mohammad Reza Seddigh,Sajjad Shokouhyar,Fateme...   \n",
              "2     Kirikkaleli, Dervis; Adebayo, Tomiwa Sunday; K...   \n",
              "3     Megan Paull,Kirsten Holmes,Maryam Omari,Debbie...   \n",
              "4     S I Khizrieva,R A Khalilov,A M Dzhafarova,V R ...   \n",
              "...                                                 ...   \n",
              "1995  Qian, Mengcheng; Yang, Linyan; Chen, Xingkui; ...   \n",
              "1996  Taciano L Milfont,Danny Osborne,Chris G Sibley...   \n",
              "1997  Esin Gizem Olgun,Sirmen Kizilcan Cetin,Zeynep ...   \n",
              "1998  Nihal Engin Vrana,Sharda Gupta,Kunal Mitra,Alb...   \n",
              "1999  Xuejiao Chang,Xiaoya Zhou,Yu Tang,Ying Zhang,J...   \n",
              "\n",
              "                                               abstract  \\\n",
              "0     Background Cardiac troponins represent a sensi...   \n",
              "1     These two main objectives of this study are to...   \n",
              "2     The main aim of this paper is to explore the r...   \n",
              "3     This paper examines myths and misconceptions a...   \n",
              "4     For evaluation of the contribution of the anti...   \n",
              "...                                                 ...   \n",
              "1995  Elevated concentrations and potential toxiciti...   \n",
              "1996  The ongoing COVID-19 pandemic claimed millions...   \n",
              "1997  We aimed to determine the prevalence of early ...   \n",
              "1998  The application of 3D printing technologies fi...   \n",
              "1999  Peanut allergy is the leading pediatric food a...   \n",
              "\n",
              "                                                   text  label  \n",
              "0     Tobacco Consumption and High-Sensitivity Cardi...      1  \n",
              "1     Approaching towards sustainable supply chain u...      1  \n",
              "2     Does globalization matter for ecological footp...      1  \n",
              "3     Myths and Misconceptions About University Stud...      1  \n",
              "4     Antioxidant Status of Rat Liver Mitochondria u...      1  \n",
              "...                                                 ...    ...  \n",
              "1995  The treatment of veterinary antibiotics in swi...      1  \n",
              "1996  Socio-political efficacy explains increase in ...      1  \n",
              "1997  Investigation of early puberty prevalence and ...      1  \n",
              "1998  From 3D printing to 3D bioprinting: the materi...      1  \n",
              "1999  Effect of Processing on the Structure and Alle...      1  \n",
              "\n",
              "[2000 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-db026c7c-b888-4721-ae4c-acfe66fee64a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>uuid</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>abstract</th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Tobacco Consumption and High-Sensitivity Cardi...</td>\n",
              "      <td>Julia Brox Skranes,Magnus Nakrem Lyngbakken,Kr...</td>\n",
              "      <td>Background Cardiac troponins represent a sensi...</td>\n",
              "      <td>Tobacco Consumption and High-Sensitivity Cardi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Approaching towards sustainable supply chain u...</td>\n",
              "      <td>Mohammad Reza Seddigh,Sajjad Shokouhyar,Fateme...</td>\n",
              "      <td>These two main objectives of this study are to...</td>\n",
              "      <td>Approaching towards sustainable supply chain u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Does globalization matter for ecological footp...</td>\n",
              "      <td>Kirikkaleli, Dervis; Adebayo, Tomiwa Sunday; K...</td>\n",
              "      <td>The main aim of this paper is to explore the r...</td>\n",
              "      <td>Does globalization matter for ecological footp...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Myths and Misconceptions About University Stud...</td>\n",
              "      <td>Megan Paull,Kirsten Holmes,Maryam Omari,Debbie...</td>\n",
              "      <td>This paper examines myths and misconceptions a...</td>\n",
              "      <td>Myths and Misconceptions About University Stud...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Antioxidant Status of Rat Liver Mitochondria u...</td>\n",
              "      <td>S I Khizrieva,R A Khalilov,A M Dzhafarova,V R ...</td>\n",
              "      <td>For evaluation of the contribution of the anti...</td>\n",
              "      <td>Antioxidant Status of Rat Liver Mitochondria u...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>1995</td>\n",
              "      <td>The treatment of veterinary antibiotics in swi...</td>\n",
              "      <td>Qian, Mengcheng; Yang, Linyan; Chen, Xingkui; ...</td>\n",
              "      <td>Elevated concentrations and potential toxiciti...</td>\n",
              "      <td>The treatment of veterinary antibiotics in swi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>1996</td>\n",
              "      <td>Socio-political efficacy explains increase in ...</td>\n",
              "      <td>Taciano L Milfont,Danny Osborne,Chris G Sibley...</td>\n",
              "      <td>The ongoing COVID-19 pandemic claimed millions...</td>\n",
              "      <td>Socio-political efficacy explains increase in ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>1997</td>\n",
              "      <td>Investigation of early puberty prevalence and ...</td>\n",
              "      <td>Esin Gizem Olgun,Sirmen Kizilcan Cetin,Zeynep ...</td>\n",
              "      <td>We aimed to determine the prevalence of early ...</td>\n",
              "      <td>Investigation of early puberty prevalence and ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>1998</td>\n",
              "      <td>From 3D printing to 3D bioprinting: the materi...</td>\n",
              "      <td>Nihal Engin Vrana,Sharda Gupta,Kunal Mitra,Alb...</td>\n",
              "      <td>The application of 3D printing technologies fi...</td>\n",
              "      <td>From 3D printing to 3D bioprinting: the materi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>1999</td>\n",
              "      <td>Effect of Processing on the Structure and Alle...</td>\n",
              "      <td>Xuejiao Chang,Xiaoya Zhou,Yu Tang,Ying Zhang,J...</td>\n",
              "      <td>Peanut allergy is the leading pediatric food a...</td>\n",
              "      <td>Effect of Processing on the Structure and Alle...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows × 6 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db026c7c-b888-4721-ae4c-acfe66fee64a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-a34c8c8e-6db1-45ef-a890-7ab23cfc3f90\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a34c8c8e-6db1-45ef-a890-7ab23cfc3f90')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-a34c8c8e-6db1-45ef-a890-7ab23cfc3f90 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-db026c7c-b888-4721-ae4c-acfe66fee64a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-db026c7c-b888-4721-ae4c-acfe66fee64a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义停用词，去掉出现较多，但对文章不关键的词语\n",
        "stops_ =[i.strip() for i in open(r'/content/drive/MyDrive/Colab Notebooks/stop.txt',encoding='utf-8').readlines()]\n",
        "stops_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocaC4FUIK1la",
        "outputId": "d811bd67-4c25-4606-9e8a-e17faafb3e7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"'d\",\n",
              " \"'ll\",\n",
              " \"'m\",\n",
              " \"'re\",\n",
              " \"'s\",\n",
              " \"'t\",\n",
              " \"'ve\",\n",
              " 'ZT',\n",
              " 'ZZ',\n",
              " 'a',\n",
              " \"a's\",\n",
              " 'able',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abst',\n",
              " 'accordance',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actually',\n",
              " 'added',\n",
              " 'adj',\n",
              " 'adopted',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affects',\n",
              " 'after',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ah',\n",
              " \"ain't\",\n",
              " 'all',\n",
              " 'allow',\n",
              " 'allows',\n",
              " 'almost',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'already',\n",
              " 'also',\n",
              " 'although',\n",
              " 'always',\n",
              " 'am',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'an',\n",
              " 'and',\n",
              " 'announce',\n",
              " 'another',\n",
              " 'any',\n",
              " 'anybody',\n",
              " 'anyhow',\n",
              " 'anymore',\n",
              " 'anyone',\n",
              " 'anything',\n",
              " 'anyway',\n",
              " 'anyways',\n",
              " 'anywhere',\n",
              " 'apart',\n",
              " 'apparently',\n",
              " 'appear',\n",
              " 'appreciate',\n",
              " 'appropriate',\n",
              " 'approximately',\n",
              " 'are',\n",
              " 'area',\n",
              " 'areas',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'arent',\n",
              " 'arise',\n",
              " 'around',\n",
              " 'as',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'associated',\n",
              " 'at',\n",
              " 'auth',\n",
              " 'available',\n",
              " 'away',\n",
              " 'awfully',\n",
              " 'b',\n",
              " 'back',\n",
              " 'backed',\n",
              " 'backing',\n",
              " 'backs',\n",
              " 'be',\n",
              " 'became',\n",
              " 'because',\n",
              " 'become',\n",
              " 'becomes',\n",
              " 'becoming',\n",
              " 'been',\n",
              " 'before',\n",
              " 'beforehand',\n",
              " 'began',\n",
              " 'begin',\n",
              " 'beginning',\n",
              " 'beginnings',\n",
              " 'begins',\n",
              " 'behind',\n",
              " 'being',\n",
              " 'beings',\n",
              " 'believe',\n",
              " 'below',\n",
              " 'beside',\n",
              " 'besides',\n",
              " 'best',\n",
              " 'better',\n",
              " 'between',\n",
              " 'beyond',\n",
              " 'big',\n",
              " 'biol',\n",
              " 'both',\n",
              " 'brief',\n",
              " 'briefly',\n",
              " 'but',\n",
              " 'by',\n",
              " 'c',\n",
              " \"c'mon\",\n",
              " \"c's\",\n",
              " 'ca',\n",
              " 'came',\n",
              " 'can',\n",
              " \"can't\",\n",
              " 'cannot',\n",
              " 'cant',\n",
              " 'case',\n",
              " 'cases',\n",
              " 'cause',\n",
              " 'causes',\n",
              " 'certain',\n",
              " 'certainly',\n",
              " 'changes',\n",
              " 'clear',\n",
              " 'clearly',\n",
              " 'co',\n",
              " 'com',\n",
              " 'come',\n",
              " 'comes',\n",
              " 'concerning',\n",
              " 'consequently',\n",
              " 'consider',\n",
              " 'considering',\n",
              " 'contain',\n",
              " 'containing',\n",
              " 'contains',\n",
              " 'corresponding',\n",
              " 'could',\n",
              " \"couldn't\",\n",
              " 'couldnt',\n",
              " 'course',\n",
              " 'currently',\n",
              " 'd',\n",
              " 'date',\n",
              " 'definitely',\n",
              " 'describe',\n",
              " 'described',\n",
              " 'despite',\n",
              " 'did',\n",
              " \"didn't\",\n",
              " 'differ',\n",
              " 'different',\n",
              " 'differently',\n",
              " 'discuss',\n",
              " 'do',\n",
              " 'does',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " \"don't\",\n",
              " 'done',\n",
              " 'down',\n",
              " 'downed',\n",
              " 'downing',\n",
              " 'downs',\n",
              " 'downwards',\n",
              " 'due',\n",
              " 'during',\n",
              " 'e',\n",
              " 'each',\n",
              " 'early',\n",
              " 'ed',\n",
              " 'edu',\n",
              " 'effect',\n",
              " 'eg',\n",
              " 'eight',\n",
              " 'eighty',\n",
              " 'either',\n",
              " 'else',\n",
              " 'elsewhere',\n",
              " 'end',\n",
              " 'ended',\n",
              " 'ending',\n",
              " 'ends',\n",
              " 'enough',\n",
              " 'entirely',\n",
              " 'especially',\n",
              " 'et',\n",
              " 'et-al',\n",
              " 'etc',\n",
              " 'even',\n",
              " 'evenly',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'everybody',\n",
              " 'everyone',\n",
              " 'everything',\n",
              " 'everywhere',\n",
              " 'ex',\n",
              " 'exactly',\n",
              " 'example',\n",
              " 'except',\n",
              " 'f',\n",
              " 'face',\n",
              " 'faces',\n",
              " 'fact',\n",
              " 'facts',\n",
              " 'far',\n",
              " 'felt',\n",
              " 'few',\n",
              " 'ff',\n",
              " 'fifth',\n",
              " 'find',\n",
              " 'finds',\n",
              " 'first',\n",
              " 'five',\n",
              " 'fix',\n",
              " 'followed',\n",
              " 'following',\n",
              " 'follows',\n",
              " 'for',\n",
              " 'former',\n",
              " 'formerly',\n",
              " 'forth',\n",
              " 'found',\n",
              " 'four',\n",
              " 'from',\n",
              " 'full',\n",
              " 'fully',\n",
              " 'further',\n",
              " 'furthered',\n",
              " 'furthering',\n",
              " 'furthermore',\n",
              " 'furthers',\n",
              " 'g',\n",
              " 'gave',\n",
              " 'general',\n",
              " 'generally',\n",
              " 'get',\n",
              " 'gets',\n",
              " 'getting',\n",
              " 'give',\n",
              " 'given',\n",
              " 'gives',\n",
              " 'giving',\n",
              " 'go',\n",
              " 'goes',\n",
              " 'going',\n",
              " 'gone',\n",
              " 'good',\n",
              " 'goods',\n",
              " 'got',\n",
              " 'gotten',\n",
              " 'great',\n",
              " 'greater',\n",
              " 'greatest',\n",
              " 'greetings',\n",
              " 'group',\n",
              " 'grouped',\n",
              " 'grouping',\n",
              " 'groups',\n",
              " 'h',\n",
              " 'had',\n",
              " \"hadn't\",\n",
              " 'happens',\n",
              " 'hardly',\n",
              " 'has',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he's\",\n",
              " 'hed',\n",
              " 'hello',\n",
              " 'help',\n",
              " 'hence',\n",
              " 'her',\n",
              " 'here',\n",
              " \"here's\",\n",
              " 'hereafter',\n",
              " 'hereby',\n",
              " 'herein',\n",
              " 'heres',\n",
              " 'hereupon',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'hes',\n",
              " 'hi',\n",
              " 'hid',\n",
              " 'high',\n",
              " 'higher',\n",
              " 'highest',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'hither',\n",
              " 'home',\n",
              " 'hopefully',\n",
              " 'how',\n",
              " 'howbeit',\n",
              " 'however',\n",
              " 'hundred',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'id',\n",
              " 'ie',\n",
              " 'if',\n",
              " 'ignored',\n",
              " 'im',\n",
              " 'immediate',\n",
              " 'immediately',\n",
              " 'importance',\n",
              " 'important',\n",
              " 'in',\n",
              " 'inasmuch',\n",
              " 'inc',\n",
              " 'include',\n",
              " 'indeed',\n",
              " 'index',\n",
              " 'indicate',\n",
              " 'indicated',\n",
              " 'indicates',\n",
              " 'information',\n",
              " 'inner',\n",
              " 'insofar',\n",
              " 'instead',\n",
              " 'interest',\n",
              " 'interested',\n",
              " 'interesting',\n",
              " 'interests',\n",
              " 'into',\n",
              " 'invention',\n",
              " 'inward',\n",
              " 'is',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'itd',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'j',\n",
              " 'just',\n",
              " 'k',\n",
              " 'keep',\n",
              " 'keeps',\n",
              " 'kept',\n",
              " 'keys',\n",
              " 'kg',\n",
              " 'kind',\n",
              " 'km',\n",
              " 'knew',\n",
              " 'know',\n",
              " 'known',\n",
              " 'knows',\n",
              " 'l',\n",
              " 'large',\n",
              " 'largely',\n",
              " 'last',\n",
              " 'lately',\n",
              " 'later',\n",
              " 'latest',\n",
              " 'latter',\n",
              " 'latterly',\n",
              " 'least',\n",
              " 'less',\n",
              " 'lest',\n",
              " 'let',\n",
              " \"let's\",\n",
              " 'lets',\n",
              " 'like',\n",
              " 'liked',\n",
              " 'likely',\n",
              " 'line',\n",
              " 'little',\n",
              " 'long',\n",
              " 'longer',\n",
              " 'longest',\n",
              " 'look',\n",
              " 'looking',\n",
              " 'looks',\n",
              " 'ltd',\n",
              " 'm',\n",
              " 'made',\n",
              " 'mainly',\n",
              " 'make',\n",
              " 'makes',\n",
              " 'making',\n",
              " 'man',\n",
              " 'many',\n",
              " 'may',\n",
              " 'maybe',\n",
              " 'me',\n",
              " 'mean',\n",
              " 'means',\n",
              " 'meantime',\n",
              " 'meanwhile',\n",
              " 'member',\n",
              " 'members',\n",
              " 'men',\n",
              " 'merely',\n",
              " 'mg',\n",
              " 'might',\n",
              " 'million',\n",
              " 'miss',\n",
              " 'ml',\n",
              " 'more',\n",
              " 'moreover',\n",
              " 'most',\n",
              " 'mostly',\n",
              " 'mr',\n",
              " 'mrs',\n",
              " 'much',\n",
              " 'mug',\n",
              " 'must',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'n',\n",
              " \"n't\",\n",
              " 'na',\n",
              " 'name',\n",
              " 'namely',\n",
              " 'nay',\n",
              " 'nd',\n",
              " 'near',\n",
              " 'nearly',\n",
              " 'necessarily',\n",
              " 'necessary',\n",
              " 'need',\n",
              " 'needed',\n",
              " 'needing',\n",
              " 'needs',\n",
              " 'neither',\n",
              " 'never',\n",
              " 'nevertheless',\n",
              " 'new',\n",
              " 'newer',\n",
              " 'newest',\n",
              " 'next',\n",
              " 'nine',\n",
              " 'ninety',\n",
              " 'no',\n",
              " 'nobody',\n",
              " 'non',\n",
              " 'none',\n",
              " 'nonetheless',\n",
              " 'noone',\n",
              " 'nor',\n",
              " 'normally',\n",
              " 'nos',\n",
              " 'not',\n",
              " 'noted',\n",
              " 'nothing',\n",
              " 'novel',\n",
              " 'now',\n",
              " 'nowhere',\n",
              " 'number',\n",
              " 'numbers',\n",
              " 'o',\n",
              " 'obtain',\n",
              " 'obtained',\n",
              " 'obviously',\n",
              " 'of',\n",
              " 'off',\n",
              " 'often',\n",
              " 'oh',\n",
              " 'ok',\n",
              " 'okay',\n",
              " 'old',\n",
              " 'older',\n",
              " 'oldest',\n",
              " 'omitted',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'only',\n",
              " 'onto',\n",
              " 'open',\n",
              " 'opened',\n",
              " 'opening',\n",
              " 'opens',\n",
              " 'or',\n",
              " 'ord',\n",
              " 'order',\n",
              " 'ordered',\n",
              " 'ordering',\n",
              " 'orders',\n",
              " 'other',\n",
              " 'others',\n",
              " 'otherwise',\n",
              " 'ought',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'outside',\n",
              " 'over',\n",
              " 'overall',\n",
              " 'owing',\n",
              " 'own',\n",
              " 'p',\n",
              " 'page',\n",
              " 'pages',\n",
              " 'part',\n",
              " 'parted',\n",
              " 'particular',\n",
              " 'particularly',\n",
              " 'parting',\n",
              " 'parts',\n",
              " 'past',\n",
              " 'per',\n",
              " 'perhaps',\n",
              " 'place',\n",
              " 'placed',\n",
              " 'places',\n",
              " 'please',\n",
              " 'plus',\n",
              " 'point',\n",
              " 'pointed',\n",
              " 'pointing',\n",
              " 'points',\n",
              " 'poorly',\n",
              " 'possible',\n",
              " 'possibly',\n",
              " 'potentially',\n",
              " 'pp',\n",
              " 'predominantly',\n",
              " 'present',\n",
              " 'presented',\n",
              " 'presenting',\n",
              " 'presents',\n",
              " 'presumably',\n",
              " 'previously',\n",
              " 'primarily',\n",
              " 'probably',\n",
              " 'problem',\n",
              " 'problems',\n",
              " 'promptly',\n",
              " 'proud',\n",
              " 'provides',\n",
              " 'put',\n",
              " 'puts',\n",
              " 'q',\n",
              " 'que',\n",
              " 'quickly',\n",
              " 'quite',\n",
              " 'qv',\n",
              " 'r',\n",
              " 'ran',\n",
              " 'rather',\n",
              " 'rd',\n",
              " 're',\n",
              " 'readily',\n",
              " 'really',\n",
              " 'reasonably',\n",
              " 'recent',\n",
              " 'recently',\n",
              " 'ref',\n",
              " 'refs',\n",
              " 'regarding',\n",
              " 'regardless',\n",
              " 'regards',\n",
              " 'related',\n",
              " 'relatively',\n",
              " 'research',\n",
              " 'respectively',\n",
              " 'resulted',\n",
              " 'resulting',\n",
              " 'results',\n",
              " 'right',\n",
              " 'room',\n",
              " 'rooms',\n",
              " 'run',\n",
              " 's',\n",
              " 'said',\n",
              " 'same',\n",
              " 'saw',\n",
              " 'say',\n",
              " 'saying',\n",
              " 'says',\n",
              " 'sec',\n",
              " 'second',\n",
              " 'secondly',\n",
              " 'seconds',\n",
              " 'section',\n",
              " 'see',\n",
              " 'seeing',\n",
              " 'seem',\n",
              " 'seemed',\n",
              " 'seeming',\n",
              " 'seems',\n",
              " 'seen',\n",
              " 'sees',\n",
              " 'self',\n",
              " 'selves',\n",
              " 'sensible',\n",
              " 'sent',\n",
              " 'serious',\n",
              " 'seriously',\n",
              " 'seven',\n",
              " 'several',\n",
              " 'shall',\n",
              " 'she',\n",
              " \"she'll\",\n",
              " 'shed',\n",
              " 'shes',\n",
              " 'should',\n",
              " \"shouldn't\",\n",
              " 'show',\n",
              " 'showed',\n",
              " 'showing',\n",
              " 'shown',\n",
              " 'showns',\n",
              " 'shows',\n",
              " 'side',\n",
              " 'sides',\n",
              " 'significant',\n",
              " 'significantly',\n",
              " 'similar',\n",
              " 'similarly',\n",
              " 'since',\n",
              " 'six',\n",
              " 'slightly',\n",
              " 'small',\n",
              " 'smaller',\n",
              " 'smallest',\n",
              " 'so',\n",
              " 'some',\n",
              " 'somebody',\n",
              " 'somehow',\n",
              " 'someone',\n",
              " 'somethan',\n",
              " 'something',\n",
              " 'sometime',\n",
              " 'sometimes',\n",
              " 'somewhat',\n",
              " 'somewhere',\n",
              " 'soon',\n",
              " 'sorry',\n",
              " 'specifically',\n",
              " 'specified',\n",
              " 'specify',\n",
              " 'specifying',\n",
              " 'state',\n",
              " 'states',\n",
              " 'still',\n",
              " 'stop',\n",
              " 'strongly',\n",
              " 'sub',\n",
              " 'substantially',\n",
              " 'successfully',\n",
              " 'such',\n",
              " 'sufficiently',\n",
              " 'suggest',\n",
              " 'sup',\n",
              " 'sure',\n",
              " 't',\n",
              " \"t's\",\n",
              " 'take',\n",
              " 'taken',\n",
              " 'taking',\n",
              " 'tell',\n",
              " 'tends',\n",
              " 'th',\n",
              " 'than',\n",
              " 'thank',\n",
              " 'thanks',\n",
              " 'thanx',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " \"that's\",\n",
              " \"that've\",\n",
              " 'thats',\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'thence',\n",
              " 'there',\n",
              " \"there'll\",\n",
              " \"there's\",\n",
              " \"there've\",\n",
              " 'thereafter',\n",
              " 'thereby',\n",
              " 'thered',\n",
              " 'therefore',\n",
              " 'therein',\n",
              " 'thereof',\n",
              " 'therere',\n",
              " 'theres',\n",
              " 'thereto',\n",
              " 'thereupon',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'theyd',\n",
              " 'theyre',\n",
              " 'thing',\n",
              " 'things',\n",
              " 'think',\n",
              " 'thinks',\n",
              " 'third',\n",
              " 'this',\n",
              " 'thorough',\n",
              " 'thoroughly',\n",
              " 'those',\n",
              " 'thou',\n",
              " 'though',\n",
              " 'thoughh',\n",
              " 'thought',\n",
              " 'thoughts',\n",
              " 'thousand',\n",
              " 'three',\n",
              " 'throug',\n",
              " 'through',\n",
              " 'throughout',\n",
              " 'thru',\n",
              " 'thus',\n",
              " 'til',\n",
              " 'tip',\n",
              " 'to',\n",
              " 'today',\n",
              " 'together',\n",
              " 'too',\n",
              " 'took',\n",
              " 'toward',\n",
              " 'towards',\n",
              " 'tried',\n",
              " 'tries',\n",
              " 'truly',\n",
              " 'try',\n",
              " 'trying',\n",
              " 'ts',\n",
              " 'turn',\n",
              " 'turned',\n",
              " 'turning',\n",
              " 'turns',\n",
              " 'twice',\n",
              " 'two',\n",
              " 'u',\n",
              " 'un',\n",
              " 'under',\n",
              " 'unfortunately',\n",
              " 'unless',\n",
              " 'unlike',\n",
              " 'unlikely',\n",
              " 'until',\n",
              " 'unto',\n",
              " 'up',\n",
              " 'upon',\n",
              " 'ups',\n",
              " 'us',\n",
              " 'use',\n",
              " 'used',\n",
              " 'useful',\n",
              " 'usefully',\n",
              " 'usefulness',\n",
              " 'uses',\n",
              " 'using',\n",
              " 'usually',\n",
              " 'uucp',\n",
              " 'v',\n",
              " 'value',\n",
              " 'various',\n",
              " 'very',\n",
              " 'via',\n",
              " 'viz',\n",
              " 'vol',\n",
              " 'vols',\n",
              " 'vs',\n",
              " 'w',\n",
              " 'want',\n",
              " 'wanted',\n",
              " 'wanting',\n",
              " 'wants',\n",
              " 'was',\n",
              " \"wasn't\",\n",
              " 'way',\n",
              " 'ways',\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'wed',\n",
              " 'welcome',\n",
              " 'well',\n",
              " 'wells',\n",
              " 'went',\n",
              " 'were',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " \"what'll\",\n",
              " \"what's\",\n",
              " 'whatever',\n",
              " 'whats',\n",
              " 'when',\n",
              " 'whence',\n",
              " 'whenever',\n",
              " 'where',\n",
              " \"where's\",\n",
              " 'whereafter',\n",
              " 'whereas',\n",
              " 'whereby',\n",
              " 'wherein',\n",
              " 'wheres',\n",
              " 'whereupon',\n",
              " 'wherever',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'whim',\n",
              " 'whither',\n",
              " 'who',\n",
              " \"who'll\",\n",
              " \"who's\",\n",
              " 'whod',\n",
              " 'whoever',\n",
              " 'whole',\n",
              " 'whom',\n",
              " 'whomever',\n",
              " 'whos',\n",
              " 'whose',\n",
              " 'why',\n",
              " 'widely',\n",
              " 'will',\n",
              " 'willing',\n",
              " 'wish',\n",
              " 'with',\n",
              " 'within',\n",
              " 'without',\n",
              " \"won't\",\n",
              " 'wonder',\n",
              " 'words',\n",
              " 'work',\n",
              " 'worked',\n",
              " 'working',\n",
              " 'works',\n",
              " 'world',\n",
              " 'would',\n",
              " \"wouldn't\",\n",
              " 'www',\n",
              " 'x',\n",
              " 'y',\n",
              " 'year',\n",
              " 'years',\n",
              " 'yes',\n",
              " 'yet',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'youd',\n",
              " 'young',\n",
              " 'younger',\n",
              " 'youngest',\n",
              " 'your',\n",
              " 'youre',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'z',\n",
              " 'zero',\n",
              " 'zt',\n",
              " 'zz',\n",
              " '!',\n",
              " '\"\"\"\"',\n",
              " '#',\n",
              " '$',\n",
              " '%',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " '+',\n",
              " '\",\"',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '?',\n",
              " '@',\n",
              " '[',\n",
              " '\\\\',\n",
              " ']',\n",
              " '^',\n",
              " '_',\n",
              " '`',\n",
              " '{',\n",
              " '|',\n",
              " '}',\n",
              " '~']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "\n",
        "# Set the NLTK data path\n",
        "nltk.data.path.append('/content/sample_data/nltk_data/corpora/stopwords')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# 下载停用词数据（如果你还没有下载过）\n",
        "nltk.download('all')\n",
        "# 定义停用词，去掉出现较多，但对文章不关键的词语\n",
        "stops__ =  set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85KHFsADj0gI",
        "outputId": "03967116-c0b2-4379-9543-cf83043b8265"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stops = list(stops__)+ stops_\n",
        "len(set(stops))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocJSyUiNoMF",
        "outputId": "ef0bc1f5-5a33-4524-a4ef-fb514c218b64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "950"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPmpHG0tJuh-",
        "outputId": "9fa479d9-c920-4211-ad87-81dde9d53e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 引入分词器\n",
        "from nltk import word_tokenize, ngrams\n",
        "# 定义方法按照词频筛选关键词\n",
        "\n",
        "def extract_keywords_by_freq(title, abstract):\n",
        "    ngrams_count = list(ngrams(word_tokenize(title.lower()), 2)) + list(ngrams(word_tokenize(abstract.lower()), 2))\n",
        "    ngrams_count = pd.DataFrame(ngrams_count)\n",
        "    ngrams_count = ngrams_count[~ngrams_count[0].isin(stops)]\n",
        "    ngrams_count = ngrams_count[~ngrams_count[1].isin(stops)]\n",
        "    ngrams_count = ngrams_count[ngrams_count[0].apply(len) > 3]\n",
        "    ngrams_count = ngrams_count[ngrams_count[1].apply(len) > 3]\n",
        "    # Assuming ngrams_count is your DataFrame with two columns: 0 and 1\n",
        "    ngrams_count['phrase'] = ngrams_count.loc[:, 0] + ' ' + ngrams_count.loc[:, 1]\n",
        "\n",
        "    ngrams_count = ngrams_count['phrase'].value_counts()\n",
        "    ngrams_count = ngrams_count[ngrams_count > 1]\n",
        "    return list(ngrams_count.index)[:5]\n",
        "\n",
        "## 对测试集提取关键词\n",
        "\n",
        "test_words = []\n",
        "for row in test.iterrows():\n",
        "    # 读取第每一行数据的标题与摘要并提取关键词\n",
        "    prediction_keywords = extract_keywords_by_freq(row[1].title, row[1].abstract)\n",
        "    # 利用文章标题进一步提取关键词\n",
        "    prediction_keywords = [x.title() for x in prediction_keywords]\n",
        "    # 如果未能提取到关键词\n",
        "    if len(prediction_keywords) == 0:\n",
        "        prediction_keywords = ['A', 'B']\n",
        "    test_words.append('; '.join(prediction_keywords))\n",
        "\n",
        "# test['Keywords'] = test_words\n",
        "# test[['uuid', 'Keywords', 'label']].to_csv('submit_task2.csv', index=None)"
      ],
      "metadata": {
        "id": "tEDrh2swj0Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test['Keywords'] = test_words\n",
        "test[['uuid', 'Keywords', 'label']].to_csv('/content/drive/MyDrive/Colab Notebooks/submit_task2.csv', index=None)"
      ],
      "metadata": {
        "id": "BL1xsbXHkmQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/KMnO4-zx/huanhuan-chat.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLBhocDmkmNj",
        "outputId": "125b42fd-4523-4aa1-e403-cd015f97bb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'huanhuan-chat'...\n",
            "remote: Enumerating objects: 1778, done.\u001b[K\n",
            "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (165/165), done.\u001b[K\n",
            "remote: Total 1778 (delta 86), reused 215 (delta 71), pack-reused 1537\u001b[K\n",
            "Receiving objects: 100% (1778/1778), 176.34 MiB | 16.71 MiB/s, done.\n",
            "Resolving deltas: 100% (91/91), done.\n",
            "Updating files: 100% (1564/1564), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data1/train.csv')\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data2/testB.csv')"
      ],
      "metadata": {
        "id": "ilMWpsz6kmKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = []\n",
        "\n",
        "for i in range(len(train_df)):\n",
        "    paper_item = train_df.loc[i]\n",
        "    tmp = {\n",
        "    \"instruction\": \"Please judge whether it is a medical field paper according to the given paper title and abstract, output 1 or 0, the following is the paper title, author and abstract -->\",\n",
        "    \"input\": f\"title:{paper_item[1]},abstract:{paper_item[3]}\",\n",
        "    \"output\": str(paper_item[5])\n",
        "  }\n",
        "    res.append(tmp)\n",
        "\n",
        "import json\n",
        "\n",
        "with open('paper_label.json', mode='w', encoding='utf-8') as f:\n",
        "    json.dump(res, f, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "JuqfPpiHkmFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./huanhuan-chat&&pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24B28mTZkmBf",
        "outputId": "980a88c6-5092-4055-fea5-626a6bc4e06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.0.1+cu118)\n",
            "Collecting transformers>=4.27.4 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.10.0 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.14.0-py3-none-any.whl (492 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.19.0 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.4.4 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.4.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.20.3)\n",
            "Collecting cpm-kernels (from -r requirements.txt (line 8))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from -r requirements.txt (line 9))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.42.1)\n",
            "Collecting rouge-chinese (from -r requirements.txt (line 11))\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (3.8.1)\n",
            "Collecting gradio (from -r requirements.txt (line 13))\n",
            "  Downloading gradio-3.39.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdtex2html (from -r requirements.txt (line 14))\n",
            "  Downloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting uvicorn (from -r requirements.txt (line 15))\n",
            "  Downloading uvicorn-0.23.1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi (from -r requirements.txt (line 16))\n",
            "  Downloading fastapi-0.100.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 17))\n",
            "  Downloading sse_starlette-1.6.1-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 1)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.1->-r requirements.txt (line 1)) (16.0.6)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers>=4.27.4->-r requirements.txt (line 2))\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (1.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers>=4.27.4->-r requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.27.4->-r requirements.txt (line 2))\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.27.4->-r requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (1.5.3)\n",
            "Collecting xxhash (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.10.0->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.10.0->-r requirements.txt (line 3)) (3.8.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.19.0->-r requirements.txt (line 4)) (5.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->-r requirements.txt (line 11)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 12)) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->-r requirements.txt (line 12)) (1.3.1)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (4.2.2)\n",
            "Collecting ffmpy (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading ffmpy-0.3.1.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.3.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading gradio_client-0.3.0-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (3.0.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson~=3.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading orjson-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (9.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from gradio->-r requirements.txt (line 13)) (1.10.11)\n",
            "Collecting pydub (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mdtex2html->-r requirements.txt (line 14)) (3.4.3)\n",
            "Collecting latex2mathml (from mdtex2html->-r requirements.txt (line 14))\n",
            "  Downloading latex2mathml-3.76.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.28.0,>=0.27.0 (from fastapi->-r requirements.txt (line 16))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.10.0->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.12.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 13)) (0.1.2)\n",
            "Requirement already satisfied: linkify-it-py<3,>=1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 13)) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (4.41.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio->-r requirements.txt (line 13)) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading mdit_py_plugins-0.3.2-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.3.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.8-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.7-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading mdit_py_plugins-0.2.6-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.5-py3-none-any.whl (39 kB)\n",
            "INFO: pip is looking at multiple versions of mdit-py-plugins to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading mdit_py_plugins-0.2.4-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.3-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.2-py3-none-any.whl (39 kB)\n",
            "  Downloading mdit_py_plugins-0.2.1-py3-none-any.whl (38 kB)\n",
            "  Downloading mdit_py_plugins-0.2.0-py3-none-any.whl (38 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading mdit_py_plugins-0.1.0-py3-none-any.whl (37 kB)\n",
            "Collecting markdown-it-py[linkify]>=2.0.0 (from gradio->-r requirements.txt (line 13))\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.10.0->-r requirements.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.27.4->-r requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi->-r requirements.txt (line 16)) (3.7.1)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r requirements.txt (line 13))\n",
            "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r requirements.txt (line 13)) (1.3.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi->-r requirements.txt (line 16)) (1.1.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio->-r requirements.txt (line 13)) (0.19.3)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r requirements.txt (line 13)) (1.0.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.1-py3-none-any.whl size=5579 sha256=1c91d85f12bb71b5de4011e304fc4a0a5a2bb9a05bc7968f8872a3a1da17b5bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/a6/d1/1c0828c304a4283b2c1639a09ad86f83d7c487ef34c6b4a1bf\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pydub, ffmpy, cpm-kernels, xxhash, websockets, semantic-version, rouge-chinese, python-multipart, orjson, markdown-it-py, latex2mathml, h11, dill, aiofiles, uvicorn, starlette, multiprocess, mdtex2html, mdit-py-plugins, huggingface-hub, httpcore, transformers, sse-starlette, httpx, fastapi, gradio-client, datasets, gradio, accelerate, trl, peft\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: mdit-py-plugins\n",
            "    Found existing installation: mdit-py-plugins 0.4.0\n",
            "    Uninstalling mdit-py-plugins-0.4.0:\n",
            "      Successfully uninstalled mdit-py-plugins-0.4.0\n",
            "Successfully installed accelerate-0.21.0 aiofiles-23.1.0 cpm-kernels-1.0.11 datasets-2.14.0 dill-0.3.7 fastapi-0.100.0 ffmpy-0.3.1 gradio-3.39.0 gradio-client-0.3.0 h11-0.14.0 httpcore-0.17.3 httpx-0.24.1 huggingface-hub-0.16.4 latex2mathml-3.76.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 multiprocess-0.70.15 orjson-3.9.2 peft-0.4.0 pydub-0.25.1 python-multipart-0.0.6 rouge-chinese-1.0.3 safetensors-0.3.1 semantic-version-2.10.0 sentencepiece-0.1.99 sse-starlette-1.6.1 starlette-0.27.0 tokenizers-0.13.3 transformers-4.31.0 trl-0.4.7 uvicorn-0.23.1 websockets-11.0.3 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/THUDM/chatglm2-6b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC6D_uSuUGdc",
        "outputId": "03c02e6d-57f9-4f06-d7c0-a8ba100a70f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'chatglm2-6b'...\n",
            "remote: Enumerating objects: 167, done.\u001b[K\n",
            "remote: Counting objects: 100% (66/66), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 167 (delta 41), reused 15 (delta 15), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (167/167), 1.94 MiB | 24.18 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "Filtering content: 100% (8/8), 11.63 GiB | 38.02 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./huanhuan-chat&&sh xfg_train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8xzS58mZue6",
        "outputId": "f2d53c3d-a965-4daf-ff58-b545696d1751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-27 08:49:53.744264: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/huanhuan-chat/src/train_bash.py\", line 21, in <module>\n",
            "    main()\n",
            "  File \"/content/huanhuan-chat/src/train_bash.py\", line 6, in main\n",
            "    model_args, data_args, training_args, finetuning_args, general_args = get_train_args()\n",
            "  File \"/content/huanhuan-chat/src/pet/core/parse.py\", line 35, in get_train_args\n",
            "    model_args, data_args, training_args, finetuning_args, general_args = parser.parse_args_into_dataclasses()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/hf_argparser.py\", line 338, in parse_args_into_dataclasses\n",
            "    obj = dtype(**inputs)\n",
            "  File \"<string>\", line 117, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\", line 1376, in __post_init__\n",
            "    raise ValueError(\n",
            "ValueError: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA devices.\n"
          ]
        }
      ]
    }
  ]
}